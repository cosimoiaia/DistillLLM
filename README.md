# DistillLama
![meme](assets/drinking_lama.jpeg)
Distill knowledge with local LLMs


Distilllama let you distill your knowledge intro your locally running LLM, using the same model for embeddings. 

Based on LangChain and LLama.cpp This work is heavily in progress.
